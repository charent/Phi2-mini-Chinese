# Phi2-Chinese-0.2B ä»0å¼€å§‹è®­ç»ƒè‡ªå·±çš„Phi2ä¸­æ–‡å°æ¨¡å‹

**æœ¬é¡¹ç›®ä¸ºå®éªŒé¡¹ç›®ï¼Œå¼€æºä»£ç åŠæ¨¡å‹æƒé‡ï¼Œé¢„è®­ç»ƒæ•°æ®è¾ƒå°‘ï¼Œå¦‚æœéœ€è¦æ•ˆæœæ›´å¥½çš„ä¸­æ–‡å°æ¨¡å‹ï¼Œå¯ä»¥å‚è€ƒé¡¹ç›®[ChatLM-mini-Chinese](https://github.com/charent/ChatLM-mini-Chinese)**

> [!CAUTION]
> æœ¬é¡¹ç›®ä¸ºå®éªŒæ€§é¡¹ç›®ï¼Œéšæ—¶ä¼šå¤§æ”¹ï¼ŒåŒ…æ‹¬è®­ç»ƒæ•°æ®ã€æ¨¡å‹ç»“æ„ã€æ–‡ä»¶ç›®å½•ç»“æ„ç­‰ã€‚
> ç¬¬ä¸€ç‰ˆæ¨¡å‹åŠè¯·æŸ¥çœ‹`tag v1.0`

- æ”¯æŒflash attention 2 åŠ é€Ÿ

# 1. âš—ï¸æ•°æ®æ¸…æ´—
æ¯”å¦‚å¥æœ«æ·»åŠ å¥å·ã€ç¹ä½“è½¬ç®€ä½“ã€åˆ é™¤é‡å¤çš„æ ‡ç‚¹ç¬¦å·ï¼ˆæ¯”å¦‚æœ‰äº›å¯¹è¯è¯­æ–™éå¸¸å¤š`"ã€‚ã€‚ã€‚ã€‚ã€‚"`ï¼‰ã€NFKC Unicodeæ ‡å‡†åŒ–ï¼ˆä¸»è¦æ˜¯å…¨è§’è½¬åŠè§’åŠç½‘é¡µæ•°æ®çš„\u3000 \xa0é—®é¢˜ï¼‰ç­‰ç­‰ã€‚   
å…·ä½“çš„æ•°æ®æ¸…æ´—è¿‡ç¨‹è¯·å‚è€ƒé¡¹ç›®[ChatLM-mini-Chinese](https://github.com/charent/ChatLM-mini-Chinese)ã€‚  
```bash
python dataset_prepare/drop_duplicate.py
```

# 2. ğŸ—¨ï¸tokenizerè®­ç»ƒ 
æœ¬é¡¹ç›®ä½¿ç”¨`byte level`çš„`BPE`åˆ†è¯å™¨ã€‚å…±æä¾›çš„ä¸¤ç§åˆ†è¯å™¨`char level` å’Œ`byte level`çš„è®­ç»ƒä»£ç ã€‚  

è®­ç»ƒå®Œçš„tokenizerè®°å¾—æ£€æŸ¥è¯è¡¨ä¸­æ˜¯å¦æœ‰å¸¸è§çš„ç‰¹æ®Šç¬¦å·ï¼Œå¦‚`\t`ã€`\n`ç­‰ï¼Œå¯ä»¥å°è¯•ç¼–ä¸€å¥åŒ…å«ç‰¹æ®Šå­—ç¬¦çš„æ–‡æœ¬`encode`ã€`decode`çœ‹çœ‹èƒ½ä¸èƒ½è¿˜åŸã€‚å¦‚æœä¸åŒ…å«è¿™äº›ç‰¹æ®Šå­—ç¬¦ï¼Œé€šè¿‡`add_tokens`å‡½æ•°æ·»åŠ ã€‚ä½¿ç”¨`len(tokenizer)`è·å–è¯è¡¨å¤§å°ï¼Œ`tokenizer.vocab_size`ä¸ç»Ÿè®¡è‡ªå·±é€šè¿‡`add_tokens`å‡½æ•°æ·»åŠ çš„å­—ç¬¦ã€‚     

tokenizerè®­ç»ƒéå¸¸åƒå†…å­˜ï¼š  

- `byte level`è®­ç»ƒ1äº¿ä¸ªå­—ç¬¦è‡³å°‘éœ€è¦`32G`å†…å­˜ï¼ˆå…¶å®`32G`è¿˜æ˜¯ä¸å¤ªå¤Ÿï¼Œä¼šé¢‘ç¹è§¦å‘swapï¼‰ï¼Œ`13600k`è®­ç»ƒæ—¶é•¿å¤§æ¦‚1ä¸ªå°æ—¶ã€‚   

- `char level`è®­ç»ƒ6.5äº¿ä¸ªå­—ç¬¦ï¼ˆåˆšå¥½æ˜¯ä¸­æ–‡wikiç™¾ç§‘çš„æ•°æ®é‡ï¼‰è‡³å°‘éœ€è¦32Gå†…å­˜ï¼Œå› ä¸ºå¤šæ¬¡è§¦å‘äº†swapï¼Œå®é™…ä½¿ç”¨é‡è¿œä¸æ­¢32Gï¼Œ`13600K`è®­ç»ƒæ—¶é•¿çº¦åŠä¸ªå°æ—¶ã€‚   

æ‰€ä»¥å¤§æ•°æ®é›†æ—¶ï¼ˆGBçº§åˆ«ï¼‰ï¼Œå»ºè®®è®­ç»ƒ`tokenizer`æ—¶ä»æ•°æ®é›†ä¸­è¿›è¡Œé‡‡æ ·ã€‚  
```bash
# ä»£ç è§ train_tokenizer/tokeinzer.ipynb
```

# 3. â›ï¸CLMå› æœæ¨¡å‹é¢„è®­ç»ƒ 

ç”¨å¤§é‡æ–‡æœ¬è¿›è¡Œæ— ç›‘ç£é¢„è®­ç»ƒï¼Œä¸»è¦ä½¿ç”¨`bell open source`çš„æ•°æ®é›†[BELLE](https://github.com/LianjiaTech/BELLE)ã€‚  

æ•°æ®é›†æ ¼å¼ï¼šä¸€ä¸ªæ ·æœ¬ä¸€å¥è¯ï¼Œå¤ªé•¿çš„å¯ä»¥æˆªæ–­åˆ†ä¸ºå¤šä¸ªæ ·æœ¬ã€‚  

CLMé¢„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹è¾“å…¥å’Œè¾“å‡ºæ˜¯ä¸€æ ·çš„ï¼Œè®¡ç®—äº¤å‰ç†µæŸå¤±çš„æ—¶å€™ï¼Œè¦é”™å¼€ä¸€ä½ï¼ˆ`shift`ï¼‰ã€‚  

å¤„ç†ç™¾ç§‘è¯­æ–™æ—¶ï¼Œå»ºè®®åœ¨æ¯ä¸ªè¯æ¡ç»“æŸååŠ ä¸Š`'[EOS]'`æ ‡è®°ã€‚å…¶ä»–è¯­æ–™å¤„ç†ä¹Ÿç±»ä¼¼ï¼Œä¸€ä¸ª`doc`çš„ç»“æŸï¼ˆå¯ä»¥æ—¶ä¸€ç¯‡æ–‡ç« ç»“æŸæˆ–è€…æ®µè½ç»“æŸï¼‰éƒ½è¦åŠ ä¸Š`'[EOS]'`æ ‡è®°ã€‚å¼€å§‹æ ‡è®°`'[BOS]'`å¯åŠ å¯ä¸åŠ ã€‚
```bash
# è¯·æ ¹æ®è‡ªå·±çš„æƒ…å†µç¼–è¾‘è„šæœ¬å‚æ•°
# win
pretrain.bat

# linux
pretrain.sh
```


# 4. âš’ï¸SFTæŒ‡ä»¤å¾®è°ƒ 

ä¸»è¦ä½¿ç”¨`bell open source`çš„æ•°æ®é›†ã€‚æ„Ÿè°¢å¤§ä½¬[BELLE](https://github.com/LianjiaTech/BELLE)ã€‚  

SFTè®­ç»ƒçš„æ•°æ®æ ¼å¼å¦‚ä¸‹ï¼š  
```python
text = f"##æé—®:\n{example['instruction']}\n##å›ç­”:\n{example['output'][EOS]"
```
æ¨¡å‹è®¡ç®—æŸå¤±æ—¶ä¼šå¿½ç•¥æ ‡è®°`"##å›ç­”:"`ä¹‹å‰çš„éƒ¨åˆ†ï¼ˆ`"##å›ç­”:"`ä¹Ÿä¼šè¢«å¿½ç•¥ï¼‰ï¼Œä»`"##å›ç­”:"`åé¢å¼€å§‹ã€‚

è®°å¾—æ·»åŠ `EOS`å¥å­ç»“æŸç‰¹æ®Šæ ‡è®°ï¼Œå¦åˆ™æ¨¡å‹`decode`çš„æ—¶å€™ä¸çŸ¥é“è¦ä»€ä¹ˆæ—¶å€™åœä¸‹æ¥ã€‚`BOS`å¥å­å¼€å§‹æ ‡è®°å¯å¡«å¯ä¸å¡«ã€‚
```bash
# è¯·æ ¹æ®è‡ªå·±çš„æƒ…å†µç¼–è¾‘è„šæœ¬å‚æ•°
# win
sft_train.bat

# linux
sft_train.sh
```


# 5. ğŸ“DPOä¼˜åŒ–

é‡‡ç”¨æ›´ç®€å•ã€æ›´èŠ‚çœæ˜¾å­˜çš„dpoåå¥½ä¼˜åŒ–æ–¹æ³•ã€‚  

æ ¹æ®ä¸ªäººå–œå¥½å¯¹SFTæ¨¡å‹å¾®è°ƒï¼Œæ•°æ®é›†è¦æ„é€ ä¸‰åˆ—`prompt`ã€`chosen`å’Œ `rejected`ï¼Œ`rejected`è¿™ä¸€åˆ—æœ‰éƒ¨åˆ†æ•°æ®æˆ‘æ˜¯ä»sfté˜¶æ®µåˆçº§æ¨¡å‹ï¼ˆæ¯”å¦‚sftè®­ç»ƒ4ä¸ª`epoch`ï¼Œå–0.5ä¸ª`epoch`æ£€æŸ¥ç‚¹çš„æ¨¡å‹ï¼‰ç”Ÿæˆï¼Œå¦‚æœç”Ÿæˆçš„`rejected`å’Œ`chosen`ç›¸ä¼¼åº¦åœ¨0.9ä»¥ä¸Šï¼Œåˆ™ä¸è¦è¿™æ¡æ•°æ®ã€‚  

DPOè¿‡ç¨‹ä¸­è¦æœ‰ä¸¤ä¸ªæ¨¡å‹ï¼Œä¸€ä¸ªæ˜¯è¦è®­ç»ƒçš„æ¨¡å‹ï¼Œä¸€ä¸ªæ˜¯å‚è€ƒçš„æ¨¡å‹ï¼Œåœ¨åŠ è½½çš„æ—¶å€™å…¶å®æ˜¯åŒä¸€ä¸ªæ¨¡å‹ï¼Œåªä¸è¿‡å‚è€ƒæ¨¡å‹ä¸å‚ä¸å‚æ•°æ›´æ–°ã€‚  
```bash
# è¯·æ ¹æ®è‡ªå·±çš„æƒ…å†µç¼–è¾‘è„šæœ¬å‚æ•°
# win
dpo_train.bat

# linux
dpo_train.sh
```


# 6. ğŸ“‘æœ¬é¡¹ç›®æ¨¡å‹ä½¿ç”¨æ–¹æ³•
## 6.1 æ™®é€šå¯¹è¯èƒ½åŠ›
æ¨¡å‹æƒé‡`huggingface`ä»“åº“ï¼š[Phi2-Chinese-0.2B](https://huggingface.co/charent/Phi2-Chinese-0.2B)  
```python
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig
import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

tokenizer = AutoTokenizer.from_pretrained('charent/Phi2-Chinese-0.2B')
model = AutoModelForCausalLM.from_pretrained('charent/Phi2-Chinese-0.2B').to(device)

txt = 'æ„Ÿå†’äº†è¦æ€ä¹ˆåŠï¼Ÿ'
prompt = f"##æé—®:\n{txt}\n##å›ç­”:\n"

# greedy search
gen_conf = GenerationConfig(
    num_beams=1,
    do_sample=False,
    max_length=320,
    max_new_tokens=256,
    no_repeat_ngram_size=4,
    eos_token_id=tokenizer.eos_token_id,
    pad_token_id=tokenizer.pad_token_id,
)

tokend = tokenizer.encode_plus(text=prompt)
input_ids, attention_mask = torch.LongTensor([tokend.input_ids]).to(device), \
    torch.LongTensor([tokend.attention_mask]).to(device)

outputs = model.generate(
    inputs=input_ids,
    attention_mask=attention_mask,
    generation_config=gen_conf,
)

outs = tokenizer.decode(outputs[0].cpu().numpy(), clean_up_tokenization_spaces=True, skip_special_tokens=True,)
print(outs)

```
```txt
##æé—®:
æ„Ÿå†’äº†è¦æ€ä¹ˆåŠï¼Ÿ
##å›ç­”:
æ„Ÿå†’æ˜¯ç”±ç—…æ¯’å¼•èµ·çš„ï¼Œæ„Ÿå†’ä¸€èˆ¬ç”±ç—…æ¯’å¼•èµ·ï¼Œä»¥ä¸‹æ˜¯ä¸€äº›å¸¸è§æ„Ÿå†’çš„æ–¹æ³•ï¼š
- æ´—æ‰‹ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¥è§¦å…¶ä»–äººæˆ–ç‰©å“åã€‚
- å’³å—½æˆ–æ‰“å–·åšæ—¶ç”¨çº¸å·¾æˆ–æ‰‹è‚˜é®ä½å£é¼»ã€‚
- ç”¨æ‰‹è§¦æ‘¸å£é¼»ï¼Œç‰¹åˆ«æ˜¯å–‰å’™å’Œé¼»å­ã€‚
- å¦‚æœå’³å—½æˆ–æ‰“å–·åšï¼Œå¯ä»¥ç”¨çº¸å·¾æˆ–æ‰‹ç»¢æ¥é®ä½å£é¼»ï¼Œä½†è¦è¿œç¦»å…¶ä»–äººã€‚
- å¦‚æœä½ æ„Ÿå†’äº†ï¼Œæœ€å¥½ä¸è¦è§¦æ‘¸è‡ªå·±çš„çœ¼ç›ã€é¼»å­å’Œå˜´å·´ã€‚
- åœ¨æ„Ÿå†’æœŸé—´ï¼Œæœ€å¥½ä¿æŒå……è¶³çš„æ°´åˆ†å’Œä¼‘æ¯ï¼Œä»¥ç¼“è§£èº«ä½“çš„ç–²åŠ³ã€‚
- å¦‚æœæ‚¨å·²ç»æ„Ÿå†’äº†ï¼Œå¯ä»¥å–ä¸€äº›æ¸©æ°´æˆ–ç›æ°´æ¥è¡¥å……ä½“æ¶²ã€‚
- å¦å¤–ï¼Œå¦‚æœæ„Ÿå†’äº†ï¼Œå»ºè®®åŠæ—¶å°±åŒ»ã€‚
```

## 6.2 æ£€ç´¢å¼ç”Ÿæˆï¼ˆRAGï¼‰
å…·ä½“ä»£ç è§`rag_with_langchain.ipynb`

![rag](./imgs/rag.png)


# 7ã€ğŸ“å¼•ç”¨
å¦‚æœä½ è§‰å¾—æœ¬é¡¹ç›®å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼Œæ¬¢è¿å¼•ç”¨ã€‚  
```conf
@misc{Charent2023,
    author={Charent Chen},
    title={A small Chinese causal language model with 0.2B parameters base on Phi2},
    year={2023},
    publisher = {GitHub},
    journal = {GitHub repository},
    howpublished = {\url{https://github.com/charent/Phi2-mini-Chinese}},
}
```

# 8ã€ğŸ¤”å…¶ä»–äº‹é¡¹
æœ¬é¡¹ç›®ä¸æ‰¿æ‹…å¼€æºæ¨¡å‹å’Œä»£ç å¯¼è‡´çš„æ•°æ®å®‰å…¨ã€èˆ†æƒ…é£é™©æˆ–å‘ç”Ÿä»»ä½•æ¨¡å‹è¢«è¯¯å¯¼ã€æ»¥ç”¨ã€ä¼ æ’­ã€ä¸å½“åˆ©ç”¨è€Œäº§ç”Ÿçš„é£é™©å’Œè´£ä»»ã€‚

